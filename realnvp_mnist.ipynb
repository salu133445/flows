{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6-final"
    },
    "colab": {
      "name": "realnvp_mnist.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salu133445/flows/blob/main/realnvp_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX8IfPomKY-z"
      },
      "source": [
        "# RealNVP on MNIST\n",
        "\n",
        "Code adapted from https://github.com/LukasRinder/normalizing-flows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92G6hUXlZ_SL"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKt_V8yO85el"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_probability as tfp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tfb = tfp.bijectors\n",
        "tfd = tfp.distributions\n",
        "tf.keras.backend.set_floatx(\"float32\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvYQ7HfRZ_SM"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EZBqmLJ85eo"
      },
      "source": [
        "def logit(z, beta=10e-6):\n",
        "    \"\"\"\n",
        "    Conversion to logit space according to equation (24) in [Papamakarios et\n",
        "    al. (2017)]. Includes scaling the input image to [0, 1] and conversion to\n",
        "    logit space.\n",
        "\n",
        "    :param z: Input tensor, e.g. image. Type: tf.float32.\n",
        "    :param beta: Small value. Default: 10e-6.\n",
        "    :return: Input tensor in logit space.\n",
        "    \"\"\"\n",
        "    inter = beta + (1 - 2 * beta) * (z / 256)\n",
        "    return tf.math.log(inter / (1 - inter))  # logit function\n",
        "\n",
        "\n",
        "def inverse_logit(x, beta=10e-6):\n",
        "    \"\"\"\n",
        "    Reverts the preprocessing steps and conversion to logit space and outputs\n",
        "    an image in range [0, 256]. Inverse of equation (24) in [Papamakarios et\n",
        "    al. (2017)].\n",
        "    \n",
        "    :param x: Input tensor in logit space. Type: tf.float32.\n",
        "    :param beta: Small value. Default: 10e-6.\n",
        "    :return: Input tensor in logit space.\n",
        "    \"\"\"\n",
        "    x = tf.math.sigmoid(x)\n",
        "    return (x - beta) * 256 / (1 - 2 * beta)\n",
        "\n",
        "def load_and_preprocess_mnist(\n",
        "    logit_space=True, batch_size=128, shuffle=True, classes=-1, channels=False\n",
        "):\n",
        "    \"\"\"\n",
        "    Loads and preprocesses the MNIST dataset. Train set: 50000, val set: 10000,\n",
        "    test set: 10000.\n",
        "\n",
        "    :param logit_space: If True, the data is converted to logit space.\n",
        "    :param batch_size: batch size\n",
        "    :param shuffle: bool. If True, dataset will be shuffled.\n",
        "    :param classes: int of class to take, defaults to -1 = ALL\n",
        "    :return: Three batched TensorFlow datasets:\n",
        "      batched_train_data, batched_val_data, batched_test_data.\n",
        "    \"\"\"\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "    # reserve last 10000 training samples as validation set\n",
        "    x_train, x_val = x_train[:-10000], x_train[-10000:]\n",
        "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
        "\n",
        "    # if logit_space: convert to logit space, else: scale to [0, 1]\n",
        "    if logit_space:\n",
        "        x_train = logit(tf.cast(x_train, tf.float32))\n",
        "        x_test = logit(tf.cast(x_test, tf.float32))\n",
        "        x_val = logit(tf.cast(x_val, tf.float32))\n",
        "        interval = 256\n",
        "    else:\n",
        "        x_train = tf.cast(x_train / 256, tf.float32)\n",
        "        x_test = tf.cast(x_test / 256, tf.float32)\n",
        "        x_val = tf.cast(x_val / 256, tf.float32)\n",
        "        interval = 1\n",
        "\n",
        "    if classes == -1:\n",
        "        pass\n",
        "    else:\n",
        "        # TODO: Extract Multiple classes: How to to the train,val split,\n",
        "        # Do we need to to a class balance???\n",
        "        x_train = np.take(x_train, tf.where(y_train == classes), axis=0)\n",
        "        x_val = np.take(x_val, tf.where(y_val == classes), axis=0)\n",
        "        x_test = np.take(x_test, tf.where(y_test == classes), axis=0)\n",
        "\n",
        "    # reshape if necessary\n",
        "    if channels:\n",
        "        x_train = tf.reshape(x_train, (x_train.shape[0], 28, 28, 1))\n",
        "        x_val = tf.reshape(x_val, (x_val.shape[0], 28, 28, 1))\n",
        "        x_test = tf.reshape(x_test, (x_test.shape[0], 28, 28, 1))\n",
        "    else:\n",
        "        x_train = tf.reshape(x_train, (x_train.shape[0], 28, 28))\n",
        "        x_val = tf.reshape(x_val, (x_val.shape[0], 28, 28))\n",
        "        x_test = tf.reshape(x_test, (x_test.shape[0], 28, 28))\n",
        "\n",
        "    if shuffle:\n",
        "        shuffled_train_data = tf.data.Dataset.from_tensor_slices(\n",
        "            x_train\n",
        "        ).shuffle(1000)\n",
        "\n",
        "    batched_train_data = shuffled_train_data.batch(batch_size)\n",
        "    batched_val_data = tf.data.Dataset.from_tensor_slices(x_val).batch(\n",
        "        batch_size\n",
        "    )\n",
        "    batched_test_data = tf.data.Dataset.from_tensor_slices(x_test).batch(\n",
        "        batch_size\n",
        "    )\n",
        "\n",
        "    return batched_train_data, batched_val_data, batched_test_data, interval"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHbZuR6h85ep"
      },
      "source": [
        "@tf.function\n",
        "def nll(distribution, data):\n",
        "    \"\"\"\n",
        "    Computes the negative log liklihood loss for a given distribution and given\n",
        "    data.\n",
        "    \n",
        "    :param distribution: TensorFlow distribution, e.g.\n",
        "      tf.TransformedDistribution.\n",
        "    :param data: Data or a batch from data.\n",
        "    :return: Negative Log Likelihodd loss.\n",
        "    \"\"\"\n",
        "    return -tf.reduce_mean(distribution.log_prob(data))\n",
        "\n",
        "@tf.function\n",
        "def train_density_estimation(distribution, optimizer, batch):\n",
        "    \"\"\"\n",
        "    Train function for density estimation normalizing flows.\n",
        "    \n",
        "    :param distribution: TensorFlow distribution, e.g.\n",
        "      tf.TransformedDistribution.\n",
        "    :param optimizer: TensorFlow keras optimizer, e.g.\n",
        "      tf.keras.optimizers.Adam.\n",
        "    :param batch: Batch of the train data.\n",
        "    :return: loss.\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(distribution.trainable_variables)\n",
        "        loss = -tf.reduce_mean(\n",
        "            distribution.log_prob(batch)\n",
        "        )  # negative log likelihood\n",
        "    gradients = tape.gradient(loss, distribution.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, distribution.trainable_variables))\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57Xux02m85eq"
      },
      "source": [
        "class NN(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Neural Network Architecture for calcualting s and t for Real-NVP\n",
        "\n",
        "    :param input_shape: shape of the data coming in the layer\n",
        "    :param hidden_units: Python list-like of non-negative integers, specifying\n",
        "      the number of units in each hidden layer.\n",
        "    :param activation: Activation of the hidden units\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, input_shape, n_hidden=[512, 512], activation=\"relu\", name=\"nn\"\n",
        "    ):\n",
        "        super().__init__(name=name)\n",
        "        layer_list = []\n",
        "        for i, hidden in enumerate(n_hidden):\n",
        "            layer_list.append(\n",
        "                tf.keras.layers.Dense(hidden, activation=activation)\n",
        "            )\n",
        "        self.layer_list = layer_list\n",
        "        self.log_s_layer = tf.keras.layers.Dense(\n",
        "            input_shape, activation=\"tanh\", name=\"log_s\")\n",
        "        self.t_layer = tf.keras.layers.Dense(input_shape, name=\"t\")\n",
        "\n",
        "    def call(self, x):\n",
        "        y = x\n",
        "        for layer in self.layer_list:\n",
        "            y = layer(y)\n",
        "        log_s = self.log_s_layer(y)\n",
        "        t = self.t_layer(y)\n",
        "        return log_s, t\n",
        "\n",
        "\n",
        "class RealNVP(tfb.Bijector):\n",
        "    \"\"\"\n",
        "    Implementation of a Real-NVP for Denisty Estimation. L. Dinh “Density\n",
        "    estimation using Real NVP,” 2016. This implementation only works for 1D\n",
        "    arrays.\n",
        "\n",
        "    :param input_shape: shape of the data coming in the layer\n",
        "    :param hidden_units: Python list-like of non-negative integers, specifying\n",
        "      the number of units in each hidden layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_shape,\n",
        "        n_hidden=[512, 512],\n",
        "        forward_min_event_ndims=1,\n",
        "        validate_args: bool = False,\n",
        "        name=\"real_nvp\",\n",
        "    ):\n",
        "        super().__init__(\n",
        "            validate_args=validate_args,\n",
        "            forward_min_event_ndims=forward_min_event_ndims,\n",
        "            name=name,\n",
        "        )\n",
        "\n",
        "        assert input_shape % 2 == 0\n",
        "        input_shape = input_shape // 2\n",
        "        nn_layer = NN(input_shape, n_hidden)\n",
        "        x = tf.keras.Input(input_shape)\n",
        "        log_s, t = nn_layer(x)\n",
        "        self.nn = tf.keras.Model(x, [log_s, t], name=\"nn\")\n",
        "\n",
        "    def _bijector_fn(self, x):\n",
        "        log_s, t = self.nn(x)\n",
        "        return tfb.AffineScalar(shift=t, log_scale=log_s)\n",
        "\n",
        "    def _forward(self, x):\n",
        "        x_a, x_b = tf.split(x, 2, axis=-1)\n",
        "        y_b = x_b\n",
        "        y_a = self._bijector_fn(x_b).forward(x_a)\n",
        "        y = tf.concat([y_a, y_b], axis=-1)\n",
        "        return y\n",
        "\n",
        "    def _inverse(self, y):\n",
        "        y_a, y_b = tf.split(y, 2, axis=-1)\n",
        "        x_b = y_b\n",
        "        x_a = self._bijector_fn(y_b).inverse(y_a)\n",
        "        x = tf.concat([x_a, x_b], axis=-1)\n",
        "        return x\n",
        "\n",
        "    def _forward_log_det_jacobian(self, x):\n",
        "        x_a, x_b = tf.split(x, 2, axis=-1)\n",
        "        return self._bijector_fn(x_b).forward_log_det_jacobian(\n",
        "            x_a, event_ndims=1\n",
        "        )\n",
        "\n",
        "    def _inverse_log_det_jacobian(self, y):\n",
        "        y_a, y_b = tf.split(y, 2, axis=-1)\n",
        "        return self._bijector_fn(y_b).inverse_log_det_jacobian(\n",
        "            y_a, event_ndims=1\n",
        "        )"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oP6uBmz3Z_SO"
      },
      "source": [
        "## Load and process data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RiB9XMw85ez",
        "outputId": "111a7926-c2d5-4d36-f052-cef5dee6791a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "category = 2\n",
        "train_data, val_data, test_data, _ = load_and_preprocess_mnist(\n",
        "    logit_space=True, batch_size=128, shuffle=True, classes=category\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQSVoJHv85e0"
      },
      "source": [
        "category = \"all\"\n",
        "train_data, val_data, test_data, _ = load_and_preprocess_mnist(\n",
        "    logit_space=True, batch_size=128, shuffle=True\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDKJt3HX85e0"
      },
      "source": [
        "mnist_shape = (28, 28, 1)\n",
        "size = 28\n",
        "input_shape = size*size\n",
        "permutation = tf.cast(\n",
        "    np.concatenate(\n",
        "        (np.arange(input_shape/2,input_shape),np.arange(0,input_shape/2))\n",
        "    ),\n",
        "    tf.int32\n",
        ")\n",
        "base_dist = tfd.MultivariateNormalDiag(loc=tf.zeros(input_shape, tf.float32))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MrkI8EOZ_SQ"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUpWGBAY85e0"
      },
      "source": [
        "n_images = 10\n",
        "dataset = \"mnist\"\n",
        "exp_number = 1\n",
        "max_epochs = 200\n",
        "layers = 5\n",
        "shape = [256, 256]\n",
        "base_lr = 1e-4\n",
        "end_lr = 1e-5"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvfoOLgkZ_SQ"
      },
      "source": [
        "## Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-p3FVipM85e1"
      },
      "source": [
        "bijectors = []\n",
        "alpha = 1e-3\n",
        "\n",
        "for i in range(layers):\n",
        "    bijectors.append(tfb.BatchNormalization())\n",
        "    bijectors.append(RealNVP(input_shape=input_shape, n_hidden=shape))\n",
        "    bijectors.append(tfb.Permute(permutation))\n",
        "    \n",
        "bijectors.append(\n",
        "    tfb.Reshape(event_shape_out=(size, size), event_shape_in=(size * size,))\n",
        ")\n",
        "\n",
        "bijector = tfb.Chain(\n",
        "    bijectors=list(reversed(bijectors)), name='chain_of_real_nvp'\n",
        ")\n",
        "\n",
        "flow = tfd.TransformedDistribution(\n",
        "    distribution=base_dist,\n",
        "    bijector=bijector\n",
        ")\n",
        "\n",
        "# number of trainable variables\n",
        "n_trainable_variables = len(flow.trainable_variables)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRuqd_Po85e1"
      },
      "source": [
        "learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "    base_lr, max_epochs, end_lr, power=0.5)\n",
        "\n",
        "checkpoint_directory = \"{}/tmp_{}_{}_{}_{}_{}\".format(\n",
        "    dataset, layers, shape[0], shape[1], exp_number, category\n",
        ")\n",
        "checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt\")\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn)\n",
        "checkpoint = tf.train.Checkpoint(optimizer=opt, model=flow)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlrS77ksZ_SR"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H_g7E1y85e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7216a4e-c015-4634-e8d7-708a2666cece"
      },
      "source": [
        "global_step = []\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "# high value to ensure that first loss < min_loss\n",
        "min_val_loss = tf.convert_to_tensor(np.inf, dtype=tf.float32)  \n",
        "min_train_loss = tf.convert_to_tensor(np.inf, dtype=tf.float32)\n",
        "min_val_epoch = 0\n",
        "min_train_epoch = 0\n",
        "delta_stop = 50  # threshold for early stopping\n",
        "\n",
        "t_start = time.time()  # start time\n",
        "\n",
        "# start training\n",
        "for i in range(max_epochs):\n",
        "\n",
        "    train_data.shuffle(buffer_size=1000)\n",
        "    batch_train_losses = []\n",
        "    for batch in train_data:\n",
        "        batch_loss = train_density_estimation(flow, opt, batch)\n",
        "        batch_train_losses.append(batch_loss)\n",
        "\n",
        "    train_loss = tf.reduce_mean(batch_train_losses)\n",
        "\n",
        "    if i % int(1) == 0:\n",
        "        batch_val_losses = []\n",
        "        for batch in val_data:\n",
        "            batch_loss = nll(flow, batch)\n",
        "            batch_val_losses.append(batch_loss)\n",
        "\n",
        "        val_loss = tf.reduce_mean(batch_val_losses)\n",
        "\n",
        "        global_step.append(i)\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        if i % 10 == 0:\n",
        "            print(\n",
        "                f\"{i:3d}, train_loss: {train_loss:12.6f}, \"\n",
        "                f\"val_loss: {val_loss:12.6f}\"\n",
        "            )\n",
        "\n",
        "        if train_loss < min_train_loss:\n",
        "            min_train_loss = train_loss\n",
        "            min_train_epoch = i\n",
        "\n",
        "        if val_loss < min_val_loss:\n",
        "            min_val_loss = val_loss\n",
        "            min_val_epoch = i\n",
        "            checkpoint.write(file_prefix=checkpoint_prefix)\n",
        "\n",
        "        # no decrease in min_val_loss for \"delta_stop epochs\"\n",
        "        elif i - min_val_epoch > delta_stop:  \n",
        "            break\n",
        "\n",
        "train_time = time.time() - t_start\n",
        "\n",
        "print(f\"Training time: {train_time}\")\n",
        "print(f\"Min val loss: {min_val_loss} at epoch: {min_val_epoch}\")\n",
        "print(f\"Last val loss: {val_loss} at epoch: {i}\")\n",
        "print(f\"Min train loss: {min_train_loss} at epoch: {min_train_epoch}\")\n",
        "print(f\"Last train loss: {train_loss} at epoch: {i}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-91dcb2668405>:68: AffineScalar.__init__ (from tensorflow_probability.python.bijectors.affine_scalar) is deprecated and will be removed after 2020-01-01.\n",
            "Instructions for updating:\n",
            "`AffineScalar` bijector is deprecated; please use `tfb.Shift(loc)(tfb.Scale(...))` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:2273: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  0, train_loss:   957.811218, val_loss:   673.377197\n",
            " 10, train_loss:  -300.077911, val_loss:  -277.723816\n",
            " 20, train_loss:  -528.386963, val_loss:  -471.972168\n",
            " 30, train_loss:  -681.952271, val_loss:  -621.193359\n",
            " 40, train_loss:  -804.846863, val_loss:  -733.237427\n",
            " 50, train_loss:  -907.644592, val_loss:  -805.661011\n",
            " 60, train_loss: -1003.911255, val_loss:  -911.258728\n",
            " 70, train_loss: -1084.212646, val_loss:  -978.546875\n",
            " 80, train_loss: -1170.360596, val_loss: -1077.561646\n",
            " 90, train_loss: -1260.787598, val_loss: -1146.900513\n",
            "100, train_loss: -1354.801025, val_loss: -1258.926270\n",
            "110, train_loss: -1435.563965, val_loss: -1342.435791\n",
            "120, train_loss: -1517.432495, val_loss: -1417.562988\n",
            "130, train_loss: -1589.625366, val_loss: -1457.560669\n",
            "140, train_loss: -1652.849243, val_loss: -1504.598633\n",
            "150, train_loss: -1707.657349, val_loss: -1554.651489\n",
            "160, train_loss: -1757.266602, val_loss: -1599.295898\n",
            "170, train_loss: -1796.387573, val_loss: -1633.131470\n",
            "180, train_loss: -1848.218140, val_loss: -1664.861938\n",
            "190, train_loss: -1884.150146, val_loss: -1699.631470\n",
            "Training time: 614.9957830905914\n",
            "Min val loss: -1732.6240234375 at epoch: 195\n",
            "Last val loss: -1718.325927734375 at epoch: 199\n",
            "Min train loss: -1931.3837890625 at epoch: 198\n",
            "Last train loss: -1925.895751953125 at epoch: 199\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP7My4NTZ_SU"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nb8RU96a85e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "974672e8-5993-4dbe-d64d-a39b40de59ff"
      },
      "source": [
        "# load best model with min validation loss\r\n",
        "checkpoint.restore(checkpoint_prefix)\r\n",
        "\r\n",
        "# perform on test dataset\r\n",
        "t_start = time.time()\r\n",
        "\r\n",
        "test_losses = []\r\n",
        "for batch in test_data:\r\n",
        "    batch_loss = nll(flow, batch)\r\n",
        "    test_losses.append(batch_loss)\r\n",
        "\r\n",
        "test_loss = tf.reduce_mean(test_losses)\r\n",
        "\r\n",
        "test_time = time.time() - t_start\r\n",
        "\r\n",
        "save_dir = \"{}/sampling_{}_{}_{}_{}/\".format(\r\n",
        "    dataset, layers, shape[0], shape[1], category\r\n",
        ")\r\n",
        "\r\n",
        "if not os.path.isdir(save_dir):\r\n",
        "    os.mkdir(save_dir)\r\n",
        "for j in range(n_images):\r\n",
        "    plt.figure()\r\n",
        "    data = flow.sample(1)\r\n",
        "    data = inverse_logit(data)\r\n",
        "    data = tf.reshape(data, (1, size, size))\r\n",
        "    plt.imshow(data[0], cmap='gray')\r\n",
        "    plt.savefig(\r\n",
        "        \"{}/{}_{}_i{}.png\".format(save_dir, exp_number, min_val_epoch, j)\r\n",
        "    )\r\n",
        "    plt.close()\r\n",
        "\r\n",
        "# remove checkpoint\r\n",
        "filelist = [f for f in os.listdir(checkpoint_directory)]\r\n",
        "for f in filelist:\r\n",
        "    os.remove(os.path.join(checkpoint_directory, f))\r\n",
        "os.removedirs(checkpoint_directory)\r\n",
        "\r\n",
        "print(f\"Test loss: {test_loss} at epoch: {i}\")\r\n",
        "print(f\"Average test log likelihood: {-test_loss} at epoch: {i}\")\r\n",
        "print(f\"Test time: {test_time}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: -1592.2265625 at epoch: 199\n",
            "Average test log likelihood: 1592.2265625 at epoch: 199\n",
            "Test time: 0.26136064529418945\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}